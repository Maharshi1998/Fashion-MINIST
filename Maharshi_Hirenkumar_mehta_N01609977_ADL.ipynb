{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSu7_LWakZUF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import classification_report\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WiBUZdnskqO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, transform=transform_train, download=True)\n",
        "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, transform=transform_test, download=True)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sgCtzMYSkarF",
        "outputId": "9fc97213-9965-4ad7-8292-ca26e963b2f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:02<00:00, 12435642.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 207542.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3910995.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 5829448.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class FashionCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
        "        self.fc1 = nn.Linear(64*13*13, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, kernel_size=2, stride=2)\n",
        "        x = x.view(-1, 64*13*13)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "model = FashionCNN()\n"
      ],
      "metadata": {
        "id": "cXSJ_uvxkfzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "Sh2vKadOklOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 275  # Increase the number of epochs\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(train_loader, 0):\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print('Epoch [%d/%d], Loss: %.4f' % (epoch+1, epochs, running_loss/len(train_loader)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wHFmpBtDksgb",
        "outputId": "3d1eb75a-8ca2-4575-9631-65e41c042764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/275], Loss: 0.5818\n",
            "Epoch [2/275], Loss: 0.3957\n",
            "Epoch [3/275], Loss: 0.3499\n",
            "Epoch [4/275], Loss: 0.3206\n",
            "Epoch [5/275], Loss: 0.3043\n",
            "Epoch [6/275], Loss: 0.2873\n",
            "Epoch [7/275], Loss: 0.2746\n",
            "Epoch [8/275], Loss: 0.2666\n",
            "Epoch [9/275], Loss: 0.2550\n",
            "Epoch [10/275], Loss: 0.2488\n",
            "Epoch [11/275], Loss: 0.2429\n",
            "Epoch [12/275], Loss: 0.2370\n",
            "Epoch [13/275], Loss: 0.2290\n",
            "Epoch [14/275], Loss: 0.2243\n",
            "Epoch [15/275], Loss: 0.2212\n",
            "Epoch [16/275], Loss: 0.2151\n",
            "Epoch [17/275], Loss: 0.2147\n",
            "Epoch [18/275], Loss: 0.2100\n",
            "Epoch [19/275], Loss: 0.2076\n",
            "Epoch [20/275], Loss: 0.2008\n",
            "Epoch [21/275], Loss: 0.1989\n",
            "Epoch [22/275], Loss: 0.1998\n",
            "Epoch [23/275], Loss: 0.1951\n",
            "Epoch [24/275], Loss: 0.1899\n",
            "Epoch [25/275], Loss: 0.1934\n",
            "Epoch [26/275], Loss: 0.1916\n",
            "Epoch [27/275], Loss: 0.1864\n",
            "Epoch [28/275], Loss: 0.1836\n",
            "Epoch [29/275], Loss: 0.1861\n",
            "Epoch [30/275], Loss: 0.1794\n",
            "Epoch [31/275], Loss: 0.1788\n",
            "Epoch [32/275], Loss: 0.1736\n",
            "Epoch [33/275], Loss: 0.1725\n",
            "Epoch [34/275], Loss: 0.1752\n",
            "Epoch [35/275], Loss: 0.1731\n",
            "Epoch [36/275], Loss: 0.1685\n",
            "Epoch [37/275], Loss: 0.1679\n",
            "Epoch [38/275], Loss: 0.1647\n",
            "Epoch [39/275], Loss: 0.1619\n",
            "Epoch [40/275], Loss: 0.1681\n",
            "Epoch [41/275], Loss: 0.1626\n",
            "Epoch [42/275], Loss: 0.1608\n",
            "Epoch [43/275], Loss: 0.1584\n",
            "Epoch [44/275], Loss: 0.1622\n",
            "Epoch [45/275], Loss: 0.1562\n",
            "Epoch [46/275], Loss: 0.1558\n",
            "Epoch [47/275], Loss: 0.1546\n",
            "Epoch [48/275], Loss: 0.1537\n",
            "Epoch [49/275], Loss: 0.1522\n",
            "Epoch [50/275], Loss: 0.1516\n",
            "Epoch [51/275], Loss: 0.1510\n",
            "Epoch [52/275], Loss: 0.1476\n",
            "Epoch [53/275], Loss: 0.1494\n",
            "Epoch [54/275], Loss: 0.1512\n",
            "Epoch [55/275], Loss: 0.1452\n",
            "Epoch [56/275], Loss: 0.1452\n",
            "Epoch [57/275], Loss: 0.1443\n",
            "Epoch [58/275], Loss: 0.1427\n",
            "Epoch [59/275], Loss: 0.1456\n",
            "Epoch [60/275], Loss: 0.1416\n",
            "Epoch [61/275], Loss: 0.1359\n",
            "Epoch [62/275], Loss: 0.1402\n",
            "Epoch [63/275], Loss: 0.1392\n",
            "Epoch [64/275], Loss: 0.1412\n",
            "Epoch [65/275], Loss: 0.1354\n",
            "Epoch [66/275], Loss: 0.1380\n",
            "Epoch [67/275], Loss: 0.1367\n",
            "Epoch [68/275], Loss: 0.1349\n",
            "Epoch [69/275], Loss: 0.1375\n",
            "Epoch [70/275], Loss: 0.1377\n",
            "Epoch [71/275], Loss: 0.1332\n",
            "Epoch [72/275], Loss: 0.1349\n",
            "Epoch [73/275], Loss: 0.1335\n",
            "Epoch [74/275], Loss: 0.1336\n",
            "Epoch [75/275], Loss: 0.1331\n",
            "Epoch [76/275], Loss: 0.1312\n",
            "Epoch [77/275], Loss: 0.1294\n",
            "Epoch [78/275], Loss: 0.1331\n",
            "Epoch [79/275], Loss: 0.1284\n",
            "Epoch [80/275], Loss: 0.1291\n",
            "Epoch [81/275], Loss: 0.1274\n",
            "Epoch [82/275], Loss: 0.1327\n",
            "Epoch [83/275], Loss: 0.1259\n",
            "Epoch [84/275], Loss: 0.1271\n",
            "Epoch [85/275], Loss: 0.1213\n",
            "Epoch [86/275], Loss: 0.1278\n",
            "Epoch [87/275], Loss: 0.1252\n",
            "Epoch [88/275], Loss: 0.1250\n",
            "Epoch [89/275], Loss: 0.1232\n",
            "Epoch [90/275], Loss: 0.1239\n",
            "Epoch [91/275], Loss: 0.1290\n",
            "Epoch [92/275], Loss: 0.1225\n",
            "Epoch [93/275], Loss: 0.1246\n",
            "Epoch [94/275], Loss: 0.1206\n",
            "Epoch [95/275], Loss: 0.1204\n",
            "Epoch [96/275], Loss: 0.1226\n",
            "Epoch [97/275], Loss: 0.1189\n",
            "Epoch [98/275], Loss: 0.1247\n",
            "Epoch [99/275], Loss: 0.1202\n",
            "Epoch [100/275], Loss: 0.1218\n",
            "Epoch [101/275], Loss: 0.1213\n",
            "Epoch [102/275], Loss: 0.1179\n",
            "Epoch [103/275], Loss: 0.1162\n",
            "Epoch [104/275], Loss: 0.1205\n",
            "Epoch [105/275], Loss: 0.1205\n",
            "Epoch [106/275], Loss: 0.1187\n",
            "Epoch [107/275], Loss: 0.1199\n",
            "Epoch [108/275], Loss: 0.1186\n",
            "Epoch [109/275], Loss: 0.1176\n",
            "Epoch [110/275], Loss: 0.1151\n",
            "Epoch [111/275], Loss: 0.1152\n",
            "Epoch [112/275], Loss: 0.1177\n",
            "Epoch [113/275], Loss: 0.1165\n",
            "Epoch [114/275], Loss: 0.1168\n",
            "Epoch [115/275], Loss: 0.1136\n",
            "Epoch [116/275], Loss: 0.1092\n",
            "Epoch [117/275], Loss: 0.1126\n",
            "Epoch [118/275], Loss: 0.1129\n",
            "Epoch [119/275], Loss: 0.1158\n",
            "Epoch [120/275], Loss: 0.1156\n",
            "Epoch [121/275], Loss: 0.1153\n",
            "Epoch [122/275], Loss: 0.1134\n",
            "Epoch [123/275], Loss: 0.1080\n",
            "Epoch [124/275], Loss: 0.1091\n",
            "Epoch [125/275], Loss: 0.1107\n",
            "Epoch [126/275], Loss: 0.1119\n",
            "Epoch [127/275], Loss: 0.1092\n",
            "Epoch [128/275], Loss: 0.1096\n",
            "Epoch [129/275], Loss: 0.1101\n",
            "Epoch [130/275], Loss: 0.1106\n",
            "Epoch [131/275], Loss: 0.1092\n",
            "Epoch [132/275], Loss: 0.1065\n",
            "Epoch [133/275], Loss: 0.1068\n",
            "Epoch [134/275], Loss: 0.1090\n",
            "Epoch [135/275], Loss: 0.1066\n",
            "Epoch [136/275], Loss: 0.1119\n",
            "Epoch [137/275], Loss: 0.1033\n",
            "Epoch [138/275], Loss: 0.1089\n",
            "Epoch [139/275], Loss: 0.1058\n",
            "Epoch [140/275], Loss: 0.1130\n",
            "Epoch [141/275], Loss: 0.1061\n",
            "Epoch [142/275], Loss: 0.1063\n",
            "Epoch [143/275], Loss: 0.1106\n",
            "Epoch [144/275], Loss: 0.1030\n",
            "Epoch [145/275], Loss: 0.1085\n",
            "Epoch [146/275], Loss: 0.1056\n",
            "Epoch [147/275], Loss: 0.1099\n",
            "Epoch [148/275], Loss: 0.1064\n",
            "Epoch [149/275], Loss: 0.1051\n",
            "Epoch [150/275], Loss: 0.1006\n",
            "Epoch [151/275], Loss: 0.1026\n",
            "Epoch [152/275], Loss: 0.1070\n",
            "Epoch [153/275], Loss: 0.1074\n",
            "Epoch [154/275], Loss: 0.1018\n",
            "Epoch [155/275], Loss: 0.1097\n",
            "Epoch [156/275], Loss: 0.1100\n",
            "Epoch [157/275], Loss: 0.1051\n",
            "Epoch [158/275], Loss: 0.1068\n",
            "Epoch [159/275], Loss: 0.1023\n",
            "Epoch [160/275], Loss: 0.1010\n",
            "Epoch [161/275], Loss: 0.1070\n",
            "Epoch [162/275], Loss: 0.1051\n",
            "Epoch [163/275], Loss: 0.1082\n",
            "Epoch [164/275], Loss: 0.1047\n",
            "Epoch [165/275], Loss: 0.1011\n",
            "Epoch [166/275], Loss: 0.1042\n",
            "Epoch [167/275], Loss: 0.1033\n",
            "Epoch [168/275], Loss: 0.1031\n",
            "Epoch [169/275], Loss: 0.1047\n",
            "Epoch [170/275], Loss: 0.1018\n",
            "Epoch [171/275], Loss: 0.1072\n",
            "Epoch [172/275], Loss: 0.1022\n",
            "Epoch [173/275], Loss: 0.1033\n",
            "Epoch [174/275], Loss: 0.0983\n",
            "Epoch [175/275], Loss: 0.1008\n",
            "Epoch [176/275], Loss: 0.1030\n",
            "Epoch [177/275], Loss: 0.1088\n",
            "Epoch [178/275], Loss: 0.1002\n",
            "Epoch [179/275], Loss: 0.0982\n",
            "Epoch [180/275], Loss: 0.1021\n",
            "Epoch [181/275], Loss: 0.0979\n",
            "Epoch [182/275], Loss: 0.0981\n",
            "Epoch [183/275], Loss: 0.0999\n",
            "Epoch [184/275], Loss: 0.1003\n",
            "Epoch [185/275], Loss: 0.0966\n",
            "Epoch [186/275], Loss: 0.1012\n",
            "Epoch [187/275], Loss: 0.1024\n",
            "Epoch [188/275], Loss: 0.0984\n",
            "Epoch [189/275], Loss: 0.0974\n",
            "Epoch [190/275], Loss: 0.0992\n",
            "Epoch [191/275], Loss: 0.0975\n",
            "Epoch [192/275], Loss: 0.1074\n",
            "Epoch [193/275], Loss: 0.0951\n",
            "Epoch [194/275], Loss: 0.0956\n",
            "Epoch [195/275], Loss: 0.0993\n",
            "Epoch [196/275], Loss: 0.0993\n",
            "Epoch [197/275], Loss: 0.0987\n",
            "Epoch [198/275], Loss: 0.0957\n",
            "Epoch [199/275], Loss: 0.0981\n",
            "Epoch [200/275], Loss: 0.0999\n",
            "Epoch [201/275], Loss: 0.0974\n",
            "Epoch [202/275], Loss: 0.0980\n",
            "Epoch [203/275], Loss: 0.0940\n",
            "Epoch [204/275], Loss: 0.0992\n",
            "Epoch [205/275], Loss: 0.0987\n",
            "Epoch [206/275], Loss: 0.0970\n",
            "Epoch [207/275], Loss: 0.0979\n",
            "Epoch [208/275], Loss: 0.0997\n",
            "Epoch [209/275], Loss: 0.0891\n",
            "Epoch [210/275], Loss: 0.0943\n",
            "Epoch [211/275], Loss: 0.1034\n",
            "Epoch [212/275], Loss: 0.0988\n",
            "Epoch [213/275], Loss: 0.0927\n",
            "Epoch [214/275], Loss: 0.0988\n",
            "Epoch [215/275], Loss: 0.0948\n",
            "Epoch [216/275], Loss: 0.0947\n",
            "Epoch [217/275], Loss: 0.0973\n",
            "Epoch [218/275], Loss: 0.0954\n",
            "Epoch [219/275], Loss: 0.0950\n",
            "Epoch [220/275], Loss: 0.0927\n",
            "Epoch [221/275], Loss: 0.0947\n",
            "Epoch [222/275], Loss: 0.0919\n",
            "Epoch [223/275], Loss: 0.0976\n",
            "Epoch [224/275], Loss: 0.1012\n",
            "Epoch [225/275], Loss: 0.0946\n",
            "Epoch [226/275], Loss: 0.0957\n",
            "Epoch [227/275], Loss: 0.0912\n",
            "Epoch [228/275], Loss: 0.0954\n",
            "Epoch [229/275], Loss: 0.0944\n",
            "Epoch [230/275], Loss: 0.0950\n",
            "Epoch [231/275], Loss: 0.0968\n",
            "Epoch [232/275], Loss: 0.0943\n",
            "Epoch [233/275], Loss: 0.0942\n",
            "Epoch [234/275], Loss: 0.0995\n",
            "Epoch [235/275], Loss: 0.0962\n",
            "Epoch [236/275], Loss: 0.0937\n",
            "Epoch [237/275], Loss: 0.0947\n",
            "Epoch [238/275], Loss: 0.0971\n",
            "Epoch [239/275], Loss: 0.0936\n",
            "Epoch [240/275], Loss: 0.0917\n",
            "Epoch [241/275], Loss: 0.0961\n",
            "Epoch [242/275], Loss: 0.0940\n",
            "Epoch [243/275], Loss: 0.0950\n",
            "Epoch [244/275], Loss: 0.0908\n",
            "Epoch [245/275], Loss: 0.0917\n",
            "Epoch [246/275], Loss: 0.0981\n",
            "Epoch [247/275], Loss: 0.0903\n",
            "Epoch [248/275], Loss: 0.0932\n",
            "Epoch [249/275], Loss: 0.0967\n",
            "Epoch [250/275], Loss: 0.0861\n",
            "Epoch [251/275], Loss: 0.0958\n",
            "Epoch [252/275], Loss: 0.0888\n",
            "Epoch [253/275], Loss: 0.0921\n",
            "Epoch [254/275], Loss: 0.0914\n",
            "Epoch [255/275], Loss: 0.0965\n",
            "Epoch [256/275], Loss: 0.0961\n",
            "Epoch [257/275], Loss: 0.0929\n",
            "Epoch [258/275], Loss: 0.0914\n",
            "Epoch [259/275], Loss: 0.0889\n",
            "Epoch [260/275], Loss: 0.0949\n",
            "Epoch [261/275], Loss: 0.0896\n",
            "Epoch [262/275], Loss: 0.0918\n",
            "Epoch [263/275], Loss: 0.0916\n",
            "Epoch [264/275], Loss: 0.1016\n",
            "Epoch [265/275], Loss: 0.0961\n",
            "Epoch [266/275], Loss: 0.0922\n",
            "Epoch [267/275], Loss: 0.0873\n",
            "Epoch [268/275], Loss: 0.0953\n",
            "Epoch [269/275], Loss: 0.0874\n",
            "Epoch [270/275], Loss: 0.0903\n",
            "Epoch [271/275], Loss: 0.0980\n",
            "Epoch [272/275], Loss: 0.0978\n",
            "Epoch [273/275], Loss: 0.0898\n",
            "Epoch [274/275], Loss: 0.0956\n",
            "Epoch [275/275], Loss: 0.0900\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in test_loader:\n",
        "        inputs, labels = data[0].to(device), data[1].to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        y_true.extend(labels.tolist())\n",
        "        y_pred.extend(predicted.tolist())\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFWl_ZiIkwqO",
        "outputId": "af1b4638-1fa4-4c32-f3b5-fcfafcaf9d6f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 91 %\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.92      0.87      1000\n",
            "           1       0.99      0.98      0.99      1000\n",
            "           2       0.80      0.93      0.86      1000\n",
            "           3       0.93      0.93      0.93      1000\n",
            "           4       0.90      0.79      0.84      1000\n",
            "           5       0.99      0.98      0.99      1000\n",
            "           6       0.81      0.70      0.75      1000\n",
            "           7       0.96      0.98      0.97      1000\n",
            "           8       0.99      0.98      0.99      1000\n",
            "           9       0.97      0.97      0.97      1000\n",
            "\n",
            "    accuracy                           0.92     10000\n",
            "   macro avg       0.92      0.92      0.92     10000\n",
            "weighted avg       0.92      0.92      0.92     10000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}